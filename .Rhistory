library(tidyverse)
library(brms)
library(tidybayes)
options(stringsAsFactors=F)
dataOG = read.csv('p0Pyk30.csv')
#make two separate data frames in long form:
## 1. for WT and p0Pyk
## 2. for p0
p0PykData = dataOG %>%
gather(key = groups, value = abundance, 1:7)
head(p0PykData)
library(tidyverse)
library(brms)
library(tidybayes)
options(stringsAsFactors=F)
dataOG = read.csv('p0Pyk30.csv')
#make two separate data frames in long form:
## 1. for WT and p0Pyk
## 2. for p0
p0PykData = dataOG %>%
gather(key = groups, value = abundance, 2:7) %>%
rename(metabolite = Normalized by OD600) %>%
head(p0PykData)
library(tidyverse)
library(brms)
library(tidybayes)
options(stringsAsFactors=F)
dataOG = read.csv('p0Pyk30.csv')
#make two separate data frames in long form:
## 1. for WT and p0Pyk
## 2. for p0
p0PykData = dataOG %>%
gather(key = groups, value = abundance, 2:7) %>%
rename(metabolite = Normalized by OD600) %>%
head(p0PykData)
install.packages("lubridate", lib="C:/Program Files/R/R-3.6.2/library")
install.packages("wordcloud", lib="C:/Program Files/R/R-3.6.2/library")
# question 4
library(tidyverse)
library(tm)
library(lubridate)
library(wordcloud)
setwd("C:/Users/oyina/src/senior_2019-2020/spring_2020/political_data_science/problem_sets/ps3")
library(lubridate)
library(wordcloud)
setwd("C:/Users/oyina/src/senior_2019-2020/spring_2020/political_data_science/problem_sets/ps3")
polls = read.csv('trump_tweets.csv')
tweets = read.csv('trump_tweets.csv')
mayors_sample = sample(blacklives.mayors, 20, replace=FALSE)
library(stringr)
#preparation
library(tidyverse)
library(stringr)
library(ggplot2)
mayors<-read_csv(file="https://raw.githubusercontent.com/jmontgomery/jmontgomery.github.io/master/PDS/Datasets/Mayors.csv")
tweets<-read_csv("Tweets.csv")
my.tweets <- tweets
##### 1
##### Using this dataset, come up with your own approach for identifying tweets
##### that mentions: Police, cops, and synonyms & Black lives matters protest movements
tweets<-rename(tweets, TwitterHandle=ScreenName)
#identifying tweets for Police, cops
police.tweets <- grep("polic|cop|law enforc|bluelives|officer", tweets$Text, ignore.case=TRUE, perl=TRUE)
police.tweets <- tweets[police.tweets, ]
#identifying tweets for Black lives matters
blacklives.tweets <- grep("black live|blacklives|blm", tweets$Text, ignore.case=TRUE, perl=TRUE)
blacklives.tweets <- tweets[blacklives.tweets, ]
#? how to identify multiple patterns ?
##### 2
##### For each mayor in the dataset, what number of tweets match those criteria.
#match the TwitterHandle with mayors data set
police.mayors <- mayors %>%
left_join(count(police.tweets, TwitterHandle), by = "TwitterHandle")
blacklives.mayors <- mayors %>%
left_join(count(blacklives.tweets, TwitterHandle), by = "TwitterHandle")
police.mayors <- police.mayors %>% mutate(Count = replace_na(n, 0))
police.mayors <- select(police.mayors, FullName, Count, Population)
blacklives.mayors <- blacklives.mayors %>% mutate(Count = replace_na(n, 0))
blacklives.mayors <- select(blacklives.mayors, FullName, Count, Population)
##### 3
##### Using the mayors data from last time,
##### show how these summaary statistics relate (if it relates)
##### to the population size of the city. (Plot)
ggplot(data=police.mayors) + geom_point(mapping = aes(x = Population, y = Count))
ggplot(data=blacklives.mayors) + geom_point(mapping = aes(x = Population, y = Count))
#preparation
library(tidyverse)
library(stringr)
library(ggplot2)
mayors<-read_csv(file="https://raw.githubusercontent.com/jmontgomery/jmontgomery.github.io/master/PDS/Datasets/Mayors.csv")
tweets<-read_csv("Tweets.csv")
my.tweets <- tweets
tweets<-rename(tweets, TwitterHandle=ScreenName)
# question 4
library(tidyverse)
library(tm)
library(lubridate)
library(wordcloud)
setwd("C:/Users/oyina/src/senior_2019-2020/spring_2020/political_data_science/problem_sets/ps3")
tweets = read.csv('trump_tweets.csv')
head(tweets$created_at)
head(str_split(tweets$created_at))
head(str_split(tweets$created_at, " "))
tweets = tweets %>%
mutate(created_at = str_split(created_at, " "))
head(tweets)
head(tweets$created_at)
head(tweets$created_at[1])
head(tweets$created_at[1,])
head(tweets$created_at)[1]
type(head(tweets$created_at))
class(head(tweets$created_at))
class(head(tweets$created_at))[1]
head(tweets$created_at)
head(tweets$created_at[1])
head(tweets$created_at[,1])
head(tweets$created_at[1,])
head(tweets$created_at)
head(tweets$created_at[2])
head(tweets$created_at)[1]
head(tweets$created_at)[1][1]
head(tweets$created_at)[1][1][1]
head(tweets$created_at)[1]
class(tweets$created_at)
head(tweets$created_at)[1]
me = head(tweets$created_at)[1]
me
me[1]
me[2]
me[1,1]
me[1,2]
me[[1]]
me[[1]][1]
vector(me)
head(tweets$created_at[[1]])
head(tweets$created_at[[1]][1])
tweets = read.csv('trump_tweets.csv')
tweets = tweets %>%1
tweets = tweets %>%
mutate(created_at = str_split(created_at, " ")) %>%
add_column(date = created_at[[1]][1])
tweets = tweets %>%
mutate(created_at = str_split(created_at, " ")) %>%
mutate(date = created_at[[1]][1])
head(tweets)
tweets$sdata<-as.Date(tweets$date, "%m/%d/%Y")
class(tweets$date)
max(tweets$date)
min(tweets$date)
tweets$date
tweets$created_at
head(tweets$created_at)
head(tweets$created_at[[1]][1])
head(tweets$created_at[[1]])
tweets = read.csv('trump_tweets.csv')
tweets$created_at
head(tweets$created_at)
tweets = read.csv('trump_tweets.csv')
tweets = tweets %>%
mutate(created_at = unlist(str_split(created_at, " "))) %>%
mutate(date = created_at[[1]][1])
head(unlist(str_split(tweets$created_at)))
head(unlist(str_split(tweets$created_at, " ")))
head(unlist(str_split(tweets$created_at, " ")))[1]
(1:nrow(tweets)%%2 == 0)
head(unlist(str_split(tweets$created_at, " ")))[(1:nrow(tweets)%%2 == 0)]
unlist(str_split(tweets$created_at, " "))[(1:nrow(tweets)%%2 == 0)]
unlist(str_split(tweets$created_at, " "))[(1:nrow(tweets)%%2 != 0)]
date_col = unlist(str_split(tweets$created_at, " "))[(1:nrow(tweets)%%2 != 0)]
unlist(str_split(tweets$created_at, " "))[(1:nrow(tweets)%%2 != 0)]
head(date_col)
date_col<-as.Date(tweets$date, "%m/%d/%Y")
date_col<-as.Date(date_col, "%m/%d/%Y")
date_col
min(date_col, na.rm = TRUE)
max(date_col, na.rm = TRUE)
tweets = read.csv('trump_tweets.csv')
tweets = tweets %>%
mutate(created_at = str_split(created_at, " ")) %>%
mutate(date = unlist(str_split(tweets$created_at, " "))[(1:nrow(tweets)%%2 != 0)]) %>%
mutate(time = unlist(str_split(tweets$created_at, " "))[(1:nrow(tweets)%%2 == 0)])
tweets$date
head(tweets$time)
min_date = min(tweets$date, na.rm = TRUE)
max_date = max(tweets$date, na.rm = TRUE)
min_date
max_date
colnames(tweets)
head(tweets$is_retweet)
# subset into retweets
tweets_new = tweets %>%
filter(!is_retweet)
nrow == tweets_new$is_retweet
nrow == sum(!tweets_new$is_retweet)
sum(!tweets_new$is_retweet)
nrow
nrow(tweets_new)
head(tweets$retweet_count)
head(tweets$favorite_count)
head(max(tweets$favorite_coun))
head(max(tweets$favorite_count))
head(tweets$favorite_count)
max(head(tweets$favorite_count))
which.is.max(head(tweets$favorite_count))
# 5 most popular tweets
tweets_pop = tweets %>%
arrage(desc(favorite_count))
# 5 most popular tweets
tweets_pop = tweets %>%
arrange(desc(favorite_count))
head(tweets)
# subset into retweets
tweets = tweets %>%
filter(!is_retweet)
head(tweets)
# 5 most popular tweets
tweets_pop = tweets %>%
arrange(desc(favorite_count))
head(tweets_pop)
most_pop_tweets = tweets_pop[1:5, "favorite_count"]
most_pop_tweets
most_pop_tweets = tweets_pop[1:5, "text"]
most_pop_tweets
# 5 most popular:
# [1] "A$AP Rocky released from prison and on his way home to the United States from Sweden. It was a Rocky Week get home ASAP A$AP!"
# [2] "https://t.co/VXeKiVzpTf"
# [3] "All is well! Missiles launched from Iran at two military bases located in Iraq. Assessment of casualties &amp; damages taking place now. So far so good! We have the most powerful and well equipped military anywhere in the world by far! I will be making a statement tomorrow morning."
# [4] "MERRY CHRISTMAS!"
# [5] "Kobe Bryant despite being one of the truly great basketball players of all time was just getting started in life. He loved his family so much and had such strong passion for the future. The loss of his beautiful daughter Gianna makes this moment even more devastating...."
# 5 most retweeted tweets
tweets_retweet = tweets %>%
arrange(desc(retweet_count))
most_retweeted_tweets
most_retweeted_tweets = tweets_retweet[1:5, "text"]
most_retweeted_tweets
text = unlist(strsplit(tweets$text, '[[:space:]]'))
head(text)
head(tweets$text)
tweets$text[1:50]
text[1:50]
text = removeNumbers(text)
head(text)
text = unlist(strsplit(tweets$text, '[[:space:]]'))
norw(text)
nrow(text)
length(text)
text = removeNumbers(text)
length(text)
text = unlist(strsplit(tweets$text, '[[:space:]]'))
text = gsub("\\d", "", text)
length(text)
text = gsub('[[:punct:]]+','', text)
length(text)
head(text)
text[1:50]
text[1:100]
text = tolower(text)
text[1:100]
text = text[!(text %in% stopwords)]
stopwords = c("see","people","new","want","one","even","must","need","done","back","just","going", "know",
"can","said","like","many","like","realdonaldtrump")
text %in% sotpwords
text %in% stopwords
text = text[!(text %in% stopwords)]
text %in% stopwords
length(text)
# cleaning tweets
text = unlist(tweets$text)
text
length(text)
stopwords_std = stopwords(kind = "SMART")
stopwords = c("see","people","new","want","one","even","must","need","done","back","just","going", "know",
"can","said","like","many","like","realdonaldtrump")
stopwords_std = stopwords(kind = "SMART")
head(stopwords_std)
stopwords = c(stopwords, stopwords_std)
length(stopwords)
# cleaning tweets
tweets$text = gsub("\\s+"," ",tweets$text)
tweets$text = gsub("\\d", "", tweets$text)
tweets$text = gsub('[[:punct:]]+','', tweets$text)
tweets$text = tolower(tweets$text)
stopwords = c("see","people","new","want","one","even","must","need","done","back","just","going", "know",
"can","said","like","many","like","realdonaldtrump")
stopwords_std = stopwords(kind = "SMART")
stopwords = c(stopwords, stopwords_std)
tweets$text = tweets$text[!(tweets$text %in% stopwords)]
# tweets$text = tweets$text[!(tweets$text %in% stopwords)]
toRemove <- paste0("\\b(", paste0(stopwords, collapse="|"), ")\\b")
toRemove
tweets$text = gsub(toRemove','', tweets$text)
tweets$text = gsub(toRemove,'', tweets$text)
# creating data
tweets_words <-  tweets %>%
select(text) %>%
unnest_tokens(word, text)
install.packages("tidytext", lib="C:/Program Files/R/R-3.6.2/library")
# creating data
library(tidytext)
tweets_words <-  tweets %>%
select(text) %>%
unnest_tokens(word, text)
words <- tweets_words %>% count(word, sort=TRUE)
# creating word cloud
set.seed(1234) # for reproducibility
wordcloud(words = df$word, freq = df$freq, min.freq = 1,
max.words=200, random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
head(words)
wordcloud(words = df$word, freq = df$n, min.freq = 1,
max.words=200, random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = df$word, n = df$n, min.freq = 1,
max.words=200, random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = words$word, freq = words$n, min.freq = 1,
max.words=200, random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wc = wordcloud(words = words$word, freq = words$n, min.freq = 1,
max.words=200, random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
wc = wordcloud(words = words$word, freq = words$n, min.freq = 3,
max.words=200, random.order=FALSE,
rot.per=0.35,
colors=brewer.pal(8, "Dark2"))
# creating document term matrix
text = tweets$text
docs = Corpus(VectorSource(text))
docs <- docs %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
dtm <- TermDocumentMatrix(docs, control =
list(weighting = weightTfIdf))
matrix <- as.matrix(dtm)
words <- sort(rowSums(matrix),decreasing=TRUE)
df <- data.frame(word = names(words),freq=words)
colanmes(df)
colnames(df)
head(dtm)
dtm
matrix
matrix
